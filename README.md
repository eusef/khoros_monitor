# Khoros Content Scraping Toolkit

This repository contains a set of powerful shell scripts designed to efficiently scrape and extract content from Khoros websites. The primary workflow involves discovering recently updated URLs via a website's sitemap and then extracting the clean, structured content from those URLs as Markdown files.

## Workflow

The toolkit is designed to be used in a two-step process:

1.  **`sitemap_scraper.sh`**: First, you discover URLs from a target website. This script can crawl a site's `robots.txt` to find its sitemaps, then parse them to find all URLs that have been updated within a specified number of days. The output is a clean text file with one URL per line.

2.  **`firecrawl_extractor.sh`**: Next, you feed the list of URLs generated by the sitemap scraper into this script. It uses the [Firecrawl API](https://firecrawl.dev) to visit each URL, extract its main content, and save it as a well-formatted Markdown file.

This pipeline is perfect for tasks like content monitoring, data analysis, or feeding website content into a knowledge base or LLM.

---

## Scripts

### 1. Sitemap Scraper (`sitemap_scraper.sh`)

This script discovers and downloads URLs from a website's sitemap(s) that have been created or updated within a specific number of days. It can automatically find sitemaps from a base URL or work with a direct link to a sitemap file.

#### **Usage**

```sh
./sitemap_scraper.sh [OPTIONS] <URL>
```

#### **Arguments**

* `URL`: A base website URL (e.g., `https://example.com`) or a direct URL to a sitemap (e.g., `https://example.com/sitemap.xml`).

#### **Options**

* `-d, --days N`: (Optional) Scrape URLs modified in the last N days. Defaults to `1` (the last 24 hours).
* `-h, --help`: Display the help message.

#### **Examples**

* **Scrape URLs from the last 24 hours by discovering sitemaps:**
    ```sh
    ./sitemap_scraper.sh [https://www.example.com](https://www.example.com)
    ```
    *This will create a file named `www.example.com_urls.txt`.*

* **Scrape URLs from the last 30 days from a specific sitemap file:**
    ```sh
    ./sitemap_scraper.sh -d 30 [https://www.example.com/sitemap.xml](https://www.example.com/sitemap.xml)
    ```

---

### 2. Firecrawl Content Extractor (`firecrawl_extractor.sh`)

This script reads a list of URLs from a text file, sends each URL to the Firecrawl API to be scraped, and saves the returned Markdown content to individual files.

#### **Prerequisites**

1.  **Dependencies**: You must have `curl` and `jq` installed.
    * **On macOS (with Homebrew):** `brew install jq`
    * **On Debian/Ubuntu:** `sudo apt-get install jq`

2.  **Firecrawl API Key**: You need a Firecrawl API key. You must export it as an environment variable before running the script.
    ```sh
    export FIRECRAWL_API_KEY="your_api_key_here"
    ```

#### **Usage**

```sh
./firecrawl_extractor.sh -i <input_file> -o <output_directory>
```

#### **Options**

* `-i, --input <file>`: Path to the input text file containing one URL per line.
* `-o, --output <dir>`: Path to the directory where Markdown files will be saved.
* `-h, --help`: Display the help message.

#### **Example**

* **Scrape all URLs from `links.txt` and save them in the `crawled_content` directory:**
    ```sh
    ./firecrawl_extractor.sh -i links.txt -o ./crawled_content
    ```

---

## Putting It All Together: Full Example

Here is how you would use both scripts in a pipeline to get all the content from `mendable.ai` that has been updated in the last week.

**Step 1: Make the scripts executable**

```sh
chmod +x sitemap_scraper.sh
chmod +x firecrawl_extractor.sh
```

**Step 2: Set your Firecrawl API Key**

```sh
export FIRECRAWL_API_KEY="your_api_key_here"
```

**Step 3: Run the Sitemap Scraper**

We'll find all URLs on `mendable.ai` that have been updated in the last 7 days.

```sh
./sitemap_scraper.sh -d 7 [https://mendable.ai](https://mendable.ai)
```

This command will run and create a file named `mendable.ai_urls.txt` in your current directory.

**Step 4: Run the Firecrawl Extractor**

Now, use the output from the previous step as the input for the extractor. We'll save the Markdown files into a new directory called `mendable_content`.

```sh
./firecrawl_extractor.sh -i mendable.ai_urls.txt -o ./mendable_content
```

The script will now process each URL from the file and save its content as a separate `.md` file inside the `mendable_content` directory.

## Contributing

Contributions are welcome! If you have suggestions for improvements or find any issues, please feel free to open an issue or submit a pull request.

## License

This project is open-source and available under the [MIT License](LICENSE).